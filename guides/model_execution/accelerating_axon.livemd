# Accelerating Axon

```elixir
Mix.install([
  {:axon, ">= 0.5.0"},
  {:exla, ">= 0.5.0"},
  {:torchx, ">= 0.5.0"},
  {:benchee, github: "akoutmos/benchee", branch: :adding_table_support},
  {:kino_benchee, github: "livebook-dev/kino_benchee"},
  {:kino, ">= 0.9.0", override: true}
])
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Using Nx Compilers in Axon

Axon is built entirely on top of Nx's numerical definitions `defn`. Functions declared with `defn` tell Nx to use *just-in-time compilation* to compile and execute the given numerical definition with an available Nx compiler. Numerical definitions enable acceleration on CPU/GPU/TPU via pluggable compilers. At the time of this writing, Nx has 2 officially supported compiler/backends on top of the default `BinaryBackend`:

1. EXLA - Acceleration via Google's [XLA project](https://www.tensorflow.org/xla)
2. TorchX - Bindings to [LibTorch](https://pytorch.org/cppdocs/)

By default, Nx and Axon run all computations using the `BinaryBackend` which is a pure Elixir implementation of various numerical routines. The `BinaryBackend` is guaranteed to run wherever an Elixir installation runs; however, it is **very** slow. Due to the computational expense of neural networks, you should basically never use the `BinaryBackend` and instead opt for one of the available accelerated libraries.

There are several ways to make use of Nx compilers from within Axon. First, create a simple model for benchmarking purposes:

```elixir
model =
  Axon.input("data")
  |> Axon.dense(32)
  |> Axon.relu()
  |> Axon.dense(1)
  |> Axon.softmax()
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"data" => nil}
  outputs: "softmax_0"
  nodes: 5
>
```

## Global and Process Level Options

By default, Axon will respect the default `defn` compilation options. You can set compilation options globally or per-process with `compiler`.

Axon will respect the global and process-level Nx backend configuration options. You can set the default backend using `backend`. Backends are covered more indepth in the second half of this example.

<!-- livebook:{"force_markdown":true} -->

```elixir
# Sets the global compilation options
Nx.Defn.global_default_options(backend: EXLA, compiler: EXLA)
# OR
Nx.Defn.global_default_options(backend: Torchx.Backend)

# Sets the process-level compilation options
Nx.Defn.default_options(backend: EXLA, compiler: EXLA)
# OR
Nx.Defn.default_options(backend: Torchx.Backend)
```

Alternatively, you can specify in opts of the function call `backend`/`compiler`

<!-- livebook:{"break_markdown":true} -->

When you call `Axon.build/2`, Axon automatically marks your initialization and forward functions as JIT compiled functions. When you invoke them, they will compile a specialized version of the function using your default compiler options:

```elixir
{inputs, _next_key} =
  Nx.Random.key(9999)
  |> Nx.Random.uniform(shape: {2, 128})

{init_fn, predict_fn} = Axon.build(model, compiler: EXLA, backend: EXLA)
params = init_fn.(inputs, %{})
predict_fn.(params, inputs)
```

<!-- livebook:{"output":true} -->

```

15:39:26.463 [info] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

15:39:26.473 [info] XLA service 0x7f3488329030 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:

15:39:26.473 [info]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Ti Laptop GPU, Compute Capability 8.6

15:39:26.473 [info] Using BFC allocator.

15:39:26.473 [info] XLA backend allocating 3605004288 bytes on device 0 for BFCAllocator.

15:39:28.272 [info] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.

```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][1]
  EXLA.Backend<cuda:0, 0.3278685746.4275699756.253533>
  [
    [1.0],
    [1.0]
  ]
>
```

Notice that the inspected tensor indicates the computation has been dispatched to EXLA and the tensor's data points to an EXLA buffer.

<!-- livebook:{"break_markdown":true} -->

If you feel like setting the global or process-level compilation options is too intrusive, you can opt for more explicit behavior in a few ways. First, you can specify the JIT compiler when you build the model:

```elixir
{init_fn, predict_fn} = Axon.build(model, compiler: EXLA, backend: EXLA)
params = init_fn.(inputs, %{})
predict_fn.(params, inputs)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][1]
  EXLA.Backend<cuda:0, 0.3278685746.4275699756.253542>
  [
    [1.0],
    [1.0]
  ]
>
```

You can also instead JIT compile functions explicitly via the `Nx.Defn.jit` or compiler-specific JIT APIs. This is useful when running benchmarks against various backends:

```elixir
{init_fn, predict_fn} = Axon.build(model, compiler: EXLA, backend: EXLA)

# These will both JIT compile with EXLA
exla_init_fn = Nx.Defn.jit(init_fn, compiler: EXLA, backend: EXLA)
exla_predict_fn = EXLA.jit(predict_fn)
```

<!-- livebook:{"output":true} -->

```
#Function<135.109794929/2 in Nx.Defn.Compiler.fun/2>
```

```elixir
Benchee.run(
  %{
    "elixir init" => fn -> init_fn.(inputs, %{}) end,
    "exla init" => fn -> exla_init_fn.(inputs, %{}) end
  },
  time: 10,
  memory_time: 5,
  warmup: 2
)
```

<!-- livebook:{"output":true} -->

```
Warning: the benchmark elixir init is using an evaluated function.
  Evaluated functions perform slower than compiled functions.
  You can move the Benchee caller to a function in a module and invoke `Mod.fun()` instead.
  Alternatively, you can move the benchmark into a benchmark.exs file and run mix run benchmark.exs

Warning: the benchmark exla init is using an evaluated function.
  Evaluated functions perform slower than compiled functions.
  You can move the Benchee caller to a function in a module and invoke `Mod.fun()` instead.
  Alternatively, you can move the benchmark into a benchmark.exs file and run mix run benchmark.exs

Operating System: Linux
CPU Information: 12th Gen Intel(R) Core(TM) i9-12900HK
Number of Available Cores: 20
Available memory: 62.47 GB
Elixir 1.14.2
Erlang 25.2.2

Benchmark suite executing with the following configuration:
warmup: 2 s
time: 10 s
memory time: 5 s
reduction time: 0 ns
parallel: 1
inputs: none specified
Estimated total run time: 34 s

Benchmarking elixir init ...
Benchmarking exla init ...

Name                  ips        average  deviation         median         99th %
elixir init        773.80        1.29 ms    ±27.03%        1.21 ms        2.82 ms
exla init          747.75        1.34 ms    ±29.51%        1.22 ms        2.89 ms

Comparison: 
elixir init        773.80
exla init          747.75 - 1.03x slower +0.0450 ms

Memory usage statistics:

Name           Memory usage
elixir init        10.38 KB
exla init          10.33 KB - 0.99x memory usage -0.05469 KB

**All measurements for memory usage were the same**
```

<!-- livebook:{"output":true} -->

```vega-lite
null
```

```elixir
Benchee.run(
  %{
    "elixir predict" => fn -> predict_fn.(params, inputs) end,
    "exla predict" => fn -> exla_predict_fn.(params, inputs) end
  },
  time: 10,
  memory_time: 5,
  warmup: 2
)
```

<!-- livebook:{"output":true} -->

```
Warning: the benchmark elixir predict is using an evaluated function.
  Evaluated functions perform slower than compiled functions.
  You can move the Benchee caller to a function in a module and invoke `Mod.fun()` instead.
  Alternatively, you can move the benchmark into a benchmark.exs file and run mix run benchmark.exs

Warning: the benchmark exla predict is using an evaluated function.
  Evaluated functions perform slower than compiled functions.
  You can move the Benchee caller to a function in a module and invoke `Mod.fun()` instead.
  Alternatively, you can move the benchmark into a benchmark.exs file and run mix run benchmark.exs

Operating System: Linux
CPU Information: 12th Gen Intel(R) Core(TM) i9-12900HK
Number of Available Cores: 20
Available memory: 62.47 GB
Elixir 1.14.2
Erlang 25.2.2

Benchmark suite executing with the following configuration:
warmup: 2 s
time: 10 s
memory time: 5 s
reduction time: 0 ns
parallel: 1
inputs: none specified
Estimated total run time: 34 s

Benchmarking elixir predict ...
Benchmarking exla predict ...

Name                     ips        average  deviation         median         99th %
exla predict          6.31 K      158.45 μs    ±52.12%      133.59 μs      427.44 μs
elixir predict        6.23 K      160.41 μs    ±55.15%      135.15 μs      447.12 μs

Comparison: 
exla predict          6.31 K
elixir predict        6.23 K - 1.01x slower +1.96 μs

Memory usage statistics:

Name              Memory usage
exla predict          11.44 KB
elixir predict        11.51 KB - 1.01x memory usage +0.0703 KB

**All measurements for memory usage were the same**
```

<!-- livebook:{"output":true} -->

```vega-lite
null
```

Notice how calls to EXLA variants are significantly faster than their Elixir counterparts. These speedups become more pronounced with more complex models and workflows.

<!-- livebook:{"break_markdown":true} -->

It's important to note that in order to use a given library as an Nx compiler, it must implement the Nx compilation behaviour. For example, you cannot invoke Torchx as an Nx compiler because it does not support JIT compilation at this time.

## Using Nx Backends in Axon

In addition to JIT-compilation, Axon also supports the usage of Nx backends. Nx backends are slightly different than Nx compilers in the sense that they do not fuse calls within numerical definitions. Backends are more eager, sacrificing a bit of performance for convenience. Torchx and EXLA both support running via backends.

<!-- livebook:{"break_markdown":true} -->

Now when you invoke model functions, it will run them with the given backend:

```elixir
{init_fn, predict_fn} = Axon.build(model, backend: Torchx.Backend)
params = init_fn.(inputs, %{})
predict_fn.(params, inputs)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][1]
  [
    [1.0],
    [1.0]
  ]
>
```

```elixir
{init_fn, predict_fn} = Axon.build(model, compiler: EXLA, backend: EXLA)
params = init_fn.(inputs, %{})
predict_fn.(params, inputs)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][1]
  EXLA.Backend<cuda:0, 0.3278685746.4275961901.179470>
  [
    [1.0],
    [1.0]
  ]
>
```

Unlike with JIT-compilation, you must set the backend at the top-level in order to invoke it. You should be careful using multiple backends in the same project as attempting to mix tensors between backends may result in strange performance bugs or errors.

With most larger models, using a JIT compiler will be more performant than using a backend.

## A Note on CPUs/GPUs/TPUs

While Nx mostly tries to standardize behavior across compilers and backends, some behaviors are backend-specific. For example, the API for choosing an acceleration platform (e.g. CUDA/ROCm/TPU) is backend-specific. You should refer to your chosen compiler or backend's documentation for information on targeting various accelerators. Typically, you only need to change a few configuration options and your code will run as-is on a chosen accelerator.
